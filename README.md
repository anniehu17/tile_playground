# tile_playground

I ran a GEMM (General Matrix Multiplication) using TileLang on an H100 GPU rented from [SFCompute](https://sfcompute.com/).

My biggest challenges were understanding Kubernetes and figuring out how to execute code on a GPU.

In an effort to demystify something that intimidated me, I broke down running a GPU workload into simple steps. This repository documents my learning process and provides a template for others looking to try similar workloads.

## File Attribution

- `tilelang-pod.yaml` and `Dockerfile`: Generated by Claude after several iterations
- `gemm.py`: Taken directly from TileLang's example code: [example_gemm.py](https://github.com/tile-ai/tilelang/blob/main/examples/gemm/example_gemm.py)

## Prerequisites

- Docker installed locally
- Access to SFCompute account (or a GPU in general)

## Step By Step Guide

For most of this experiment, I used Claude to guide me through the process, demonstrating that the barrier to entry is lower than it might seem when assisted by an LLM.

Additionally, a lot of the Kubernetes commands that I ran I learned from [@devgerred](https://x.com/devgerred).

1. Publish a Docker image

Build a Docker image containing all dependencies:
```
docker build --platform linux/amd64 -t [username]/tilelang-gemm:latest .
```

Publish the image to the Docker Hub registry:
```
docker push [username]/tilelang-gemm:latest
```

2. Get access to a GPU

I rented computing resources from SFCompute via the [CLI](https://docs.sfcompute.com/docs/getting-started):
```
sf buy -n 8 -d 1h
```

Identify your assigned cluster:
```
sf clusters ls
```

Configure user access to the cluster:
```
sf clusters users add --cluster alamo --user myuser
```

Verify connection:
```
kubectl get pods
```

3. Run the Workload!

Deploy the Kubernetes job:
```
kubectl apply -f tilelang-pod.yaml
```

Monitor the job status:
```
kubectl get pods
kubectl get jobs
kubectl logs [pod-name] -f      # -f for follow to stream the logs
kubectl describe pod [pod-name]
```

The output in your terminal should display matrices and generated CUDA source code.

![Screenshot 2025-05-02 at 8 18 46â€¯PM](https://github.com/user-attachments/assets/d3e1bf56-4b9d-4c94-9c3d-f7ad38813aad)


## How I Got Here

### Code Access Methods

GPU code can be made accessible in several ways:
- Git clone during pod initialization
- Custom Docker images (the approach used here)
- Mounting code repositories as volumes

I initially tested directly on the GPU by:
- Getting a shell on the GPU pod: `kubectl exec -it tilelang-gemm -- bash`
- Manually installing dependencies (PyTorch, TileLang)
- Creating the gemm.py file and running it

I liked this manual approach because it helped me understand the process before automating with Docker.

I did run into trouble with Docker with making sure it was built for the correct platform (linux/amd64) rather than my local MacOS architecture.

### Logging

When running commands via `kubectl exec`, logs weren't accessible through `kubectl logs [pod-name] -f` because they were generated by child processes. I solved this by redirecting output:
```
python3 gemm.py > /proc/1/fd/1 2> /proc/1/fd/2
```

### Job vs. Pod

I chose to implement a Kubernetes Job rather than a persistent Pod because Jobs run tasks to completion and then stop, which made sense since we are running a one-off task here.

Also, I have not used Jobs before so I wanted to try it.

## How The Pieces Connect

I asked Claude to generate an explanation of how this overall process works:

Running GPU workloads involves several interconnected components that work together:

1. **Code Preparation**: GEMM code written in Python using TileLang, which optimizes matrix operations for GPU execution

2. **Containerization**: Code and dependencies packaged in a Docker container for consistency across environments

3. **Container Registry**: Docker image published to a public registry (docker.io) for accessibility

4. **GPU Access**: SFCompute provides H100 GPU hardware

5. **Orchestration**: Kubernetes handles deployment, resource allocation, and execution management

6. **Execution**: The container runs on the GPU, executing the TileLang code and returning results
